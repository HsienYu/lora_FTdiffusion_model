# LoRA Base Configuration for Stable Diffusion Fine-tuning

# Model Configuration
model:
  name: "runwayml/stable-diffusion-v1-5"  # Base model
  variant: "fp16"  # Use fp16 for memory efficiency
  cache_dir: "./models/base"
  
# LoRA Configuration
lora:
  r: 16  # Rank of adaptation
  alpha: 32  # LoRA scaling parameter
  dropout: 0.1
  target_modules:
    - "to_k"
    - "to_q"
    - "to_v"
    - "to_out.0"
    - "ff.net.0.proj"
    - "ff.net.2"
  bias: "none"
  task_type: "DIFFUSION"

# Training Configuration
training:
  output_dir: "./models/checkpoints"
  num_train_epochs: 100
  train_batch_size: 4
  eval_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 1e-4
  lr_scheduler: "cosine"
  warmup_steps: 500
  max_train_steps: 10000
  validation_steps: 500
  save_steps: 1000
  logging_steps: 100
  
  # Optimization
  mixed_precision: "fp16"
  gradient_checkpointing: true
  use_8bit_adam: true
  
  # Regularization
  max_grad_norm: 1.0
  weight_decay: 0.01
  
# Dataset Configuration
dataset:
  train_data_dir: "./data/processed/train"
  validation_data_dir: "./data/processed/val"
  resolution: 512
  center_crop: true
  random_flip: true
  max_train_samples: null
  dataloader_num_workers: 4

# Streaming Optimization
streaming:
  enable_optimization: true
  memory_efficient_attention: true
  use_cpu_offload: false
  enable_xformers: true
  
# Logging and Monitoring
logging:
  use_wandb: true
  wandb_project: "lora-stable-diffusion"
  report_to: ["wandb", "tensorboard"]
  log_images: true
  num_validation_images: 4

# Hardware Configuration
hardware:
  device: "cuda"
  num_gpus: 1
  mixed_precision: "fp16"
  pin_memory: true
